{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ea51fe",
   "metadata": {},
   "source": [
    "**Yahoo Finance Scrapping and scrapping cleaning** \n",
    "\n",
    "Here is where I will take the data from Yahoo Finance using selenium. \n",
    "The Jupyter is designed for the user to select which stock they want to analyze and then run the code. \n",
    "The code will run as follows:\n",
    "    \n",
    "    1. Import necessary libraries\n",
    "    2. Declare variables for Options for the Selenium web scrapping.\n",
    "    3. Import support files:\n",
    "        a. df_companies = a csv containing ticker and respective company from the SEC (followed cleaning)\n",
    "        b. df_other = a csv containing ticker and respective commodity, index or crypto\n",
    "    4. User interaction:\n",
    "        a. Request if Company or Other\n",
    "        b. Select desired Company or Other\n",
    "    5. Cleaning of the web scrapping - see details below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0136613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sidetable #bonus\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f3b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import src.functions_project as functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b4a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "opciones= Options()\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "#hide as a robot\n",
    "opciones.add_experimental_option('useAutomationExtension', False)\n",
    "opciones.add_argument('--start-maximized') #start maximized\n",
    "opciones.add_argument('user.data-dir=selenium') #save cookies\n",
    "opciones.add_argument('--incognito')#incognito window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fb9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_companies = pd.read_csv('../data/companiesclean.csv')\n",
    "df_other = pd.read_csv('../data/other_ticker.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c22fd2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to analyze a Company or Other(Index/Commodity/Crypto)?: other\n",
      "        Type                         Other   ticker\n",
      "0      Index  Dow Jones Industrial Average     ^DJI\n",
      "1      Index                       S&P 500    ^GSPC\n",
      "2      Index              NASDAQ Composite    ^IXIC\n",
      "3      Index         CBOE Volatility Index     ^VIX\n",
      "4  Commodity                          Gold     GC=F\n",
      "5  Commodity                     Crude Oil     CL=F\n",
      "6  Commodity                        Silver     SI=F\n",
      "7     Crypto                   Bitcoin USD  BTC-USD\n",
      "8     Crypto                   Etherun USD  ETH-USD\n",
      "-------------------------------------------------------------------\n",
      "Which of the following other?(copy/paste desired ticker): ^GSPC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Let the scrapping begin for ^GSPC\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current google-chrome version is 99.0.4844\n",
      "Get LATEST chromedriver version for 99.0.4844 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/99.0.4844.51/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\anton\\.wdm\\drivers\\chromedriver\\win32\\99.0.4844.51]\n"
     ]
    }
   ],
   "source": [
    "functions.scrapping_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ed8cb",
   "metadata": {},
   "source": [
    "**Scrapping** of Yahoo Finance:\n",
    "    \n",
    "    1. Declare the initial URL, home page of Yahoo Finance\n",
    "    2. Declare an empty table 'tabla' where we will drop our data\n",
    "    3. Accept cookies \n",
    "    4. IF formula based on the user answer, the user will either select Company or Other\n",
    "        a. this selection will vary the element of searcing, for example if the user selects company the search will come from         'ticker' while on the other hand, if it comes from Other, then the selection will come from 'other'\n",
    "    5. Get to the historical data page\n",
    "    6. Select \"MAX\" in the calendar, which will allow to the whole table to expand. \n",
    "    7. Very important are the scrolling down. A while True loop has been given to scroll to the end of the page 17 times using        action chains\n",
    "    8. Table is retreived and appended in the 'tabla' previously empty declared.\n",
    "    9. This is then saven in da_tabla = pd.DataFrame(tabla)\n",
    "                             da_tabla2 = da_tabla.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19226f57",
   "metadata": {},
   "source": [
    "**Cleaning of Data** Now that we have extracted our data, we need to clean it. Below are the following steps:\n",
    "    \n",
    "    1. Declare a new dataframe 'df' using loc to locate the first element of tabla and split by the page separator \"\\n\"\n",
    "        a. note only rows 6:-2 are taken because the other were informative of the website, not only the table, not needed.\n",
    "    2. Create a combined date column which is df[9], a combination of the 3 date columns.\n",
    "    3. Change column names to relevant and copying Yahoo Finance, while drop() on the ones used for d[9]\n",
    "    4. Using lambda and replace to change all the commas to spaces, important for changing types.\n",
    "    5. Change the format of 'date' to datetime\n",
    "    6. UH OH we found a trouble, check below ! check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac6d7d",
   "metadata": {},
   "source": [
    "**NOTE THIS CLEANING IS A FORMULA IN THE SRC CALLED defclean_scrapping()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ef020",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_tabla = pd.DataFrame(tabla)\n",
    "da_tabla2 = da_tabla.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c19f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(da_tabla2.loc[0,0].split(\"\\n\")[6:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e82760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec245ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st = df[0].str.split(\" \", expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0928bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st[9] = df_st[0] + ' ' + df_st[1] + ' ' + df_st[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d1cb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_st.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86214170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.columns = ['d1', 'd2', 'd3', 'Open','High','Low','Close','Adj Close','Volume','date']\n",
    "df_st.drop(['d1','d2','d3'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b68b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64afa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st = df_st.apply(lambda x: x.str.replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514efdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st['date'] = pd.to_datetime(df_st['date'], format='%b %d %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb444c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.duplicated(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f52361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27cdb8",
   "metadata": {},
   "source": [
    "**When a stock pays dividend, that day is recorded twice in the historical data\n",
    "but the values are 0, hence the 33 duplicates in the date. StockX has paid dividends 33times. \n",
    "Since I dont care about those days, because the day-data is there\n",
    "I will create a new df called df_stocks, which will be the rows that are not null in the \"close\" column.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdfb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks = df_st.loc[(~df_st['Close'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e267a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let´s check if it took out those. \n",
    "\n",
    "df_stocks.duplicated(['date']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8465f",
   "metadata": {},
   "source": [
    "**Note** Okay better, but the numbers are impossible to use because they are strings, so will transform to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fef5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks['Open'] = df_stocks['Open'].astype(float)\n",
    "df_stocks['High'] = df_stocks['High'].astype(float)\n",
    "df_stocks['Low'] = df_stocks['Low'].astype(float)\n",
    "df_stocks['Close'] = df_stocks['Close'].astype(float)\n",
    "df_stocks['Adj Close'] = df_stocks['Adj Close'].astype(float)\n",
    "df_stocks['Volume'] = df_stocks['Volume'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570bd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990d1b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_stocks.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e3245",
   "metadata": {},
   "source": [
    "**NOW** I will bring the csv for the clean tweets and declare it as df_twt, then:\n",
    "    \n",
    "    1. Change the 'time' column of the df_twt into 'date' and into datetime format.\n",
    "    2. Merge my previous df_stocks with df_twt using the date as unifier, creating df_atwt\n",
    "    3. Double check the characteristics of the file\n",
    "    4. Save new file for analysis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b376f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twt = pd.read_csv('tweetsclean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d329d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twt.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twt['date'] = pd.to_datetime(df_twt['fecha'])\n",
    "df_twt.drop(['fecha'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443510f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atwt = pd.merge(df_twt,df_stocks, how = 'outer', on = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa4c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atwt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atwt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63177b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si el mercado no está abierto, es fin de semana\n",
    "df_atwt['market_op_cl'] = np.where(df_atwt['Open'] > 0, 'market_open', 'market_closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da95311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atwt.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atwt.stb.freq(['market_op_cl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b2aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_atwt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atwt.to_csv('analysis1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1b527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
